{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbb_OgCJeUmb",
        "outputId": "39b53a44-214f-4f83-90c6-82f8ec70ee4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "#############################################\n",
        "# Cross Entropy with ResNet-34\n",
        "#############################################\n",
        "\n",
        "!pip install wandb\n",
        "\n",
        "# Imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb  # Importa WandB\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Asegúrate de que los siguientes caminos sean correctos en tu entorno de Colab\n",
        "TRAIN_CSV_PATH = '/content/drive/MyDrive/XNAPproject-grup_15-main/datasets/afad_train.csv'\n",
        "TEST_CSV_PATH = '/content/drive/MyDrive/XNAPproject-grup_15-main/datasets/afad_test.csv'\n",
        "IMAGE_PATH = '/content/drive/MyDrive/XNAPproject-grup_15-main/AFAD-Full'\n",
        "\n",
        "\n",
        "# Variables en lugar de argparse\n",
        "CUDA_DEVICE = 0  # 0 para usar la primera GPU, -1 para CPU\n",
        "NUM_WORKERS = 3\n",
        "RANDOM_SEED = 42\n",
        "OUT_PATH = '/content/output'\n",
        "\n",
        "# Crear el directorio de salida si no existe\n",
        "if not os.path.exists(OUT_PATH):\n",
        "    os.makedirs(OUT_PATH)\n",
        "LOGFILE = os.path.join(OUT_PATH, 'training.log')\n",
        "TEST_PREDICTIONS = os.path.join(OUT_PATH, 'test_predictions.log')\n",
        "\n",
        "# Inicializa WandB\n",
        "wandb.login(key='eb28261147123809156d0add6be5ba7f687a0514')\n",
        "\n",
        "wandb.init()\n",
        "\n",
        "# Logging\n",
        "header = []\n",
        "header.append('PyTorch Version: %s' % torch.__version__)\n",
        "header.append('CUDA device available: %s' % torch.cuda.is_available())\n",
        "header.append('Using CUDA device: %s' % ('cuda:%d' % CUDA_DEVICE if CUDA_DEVICE >= 0 else 'cpu'))\n",
        "header.append('Random Seed: %s' % RANDOM_SEED)\n",
        "header.append('Output Path: %s' % OUT_PATH)\n",
        "\n",
        "with open(LOGFILE, 'w') as f:\n",
        "    for entry in header:\n",
        "        print(entry)\n",
        "        f.write('%s\\n' % entry)\n",
        "        f.flush()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "60SgKTAinVfE",
        "outputId": "0f147d1a-1987-4867-bb93-4ea83473077c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.1.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240514_154012-wmzvi0ao</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eduard_ara/uncategorized/runs/wmzvi0ao' target=\"_blank\">sunny-hill-1</a></strong> to <a href='https://wandb.ai/eduard_ara/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eduard_ara/uncategorized' target=\"_blank\">https://wandb.ai/eduard_ara/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eduard_ara/uncategorized/runs/wmzvi0ao' target=\"_blank\">https://wandb.ai/eduard_ara/uncategorized/runs/wmzvi0ao</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.2.1+cu121\n",
            "CUDA device available: True\n",
            "Using CUDA device: cuda:0\n",
            "Random Seed: 42\n",
            "Output Path: /content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# SETTINGS\n",
        "##########################\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 200\n",
        "\n",
        "# Architecture\n",
        "NUM_CLASSES = 26\n",
        "BATCH_SIZE = 256\n",
        "GRAYSCALE = False"
      ],
      "metadata": {
        "id": "ufzxC0cqnZnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "# Dataset\n",
        "###################\n",
        "\n",
        "class AFADDatasetAge(Dataset):\n",
        "    \"\"\"Custom Dataset for loading AFAD face images\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "\n",
        "        df = pd.read_csv(csv_path, index_col=0)\n",
        "        self.img_dir = img_dir\n",
        "        self.csv_path = csv_path\n",
        "        self.img_paths = df['path']\n",
        "        self.y = df['age'].values\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(os.path.join(self.img_dir,\n",
        "                                      self.img_paths[index]))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = self.y[index]\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.y.shape[0]\n",
        "\n",
        "custom_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
        "                                       transforms.RandomCrop((120, 120)),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "train_dataset = AFADDatasetAge(csv_path=TRAIN_CSV_PATH,\n",
        "                               img_dir=IMAGE_PATH,\n",
        "                               transform=custom_transform)\n",
        "\n",
        "\n",
        "custom_transform2 = transforms.Compose([transforms.Resize((128, 128)),\n",
        "                                        transforms.CenterCrop((120, 120)),\n",
        "                                        transforms.ToTensor()])\n",
        "\n",
        "test_dataset = AFADDatasetAge(csv_path=TEST_CSV_PATH,\n",
        "                              img_dir=IMAGE_PATH,\n",
        "                              transform=custom_transform2)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False,\n",
        "                         num_workers=NUM_WORKERS)\n",
        "\n",
        "# Verificación de GPU\n",
        "assert torch.cuda.is_available(), \"GPU is not enabled\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iZtpoKGne4r",
        "outputId": "80e0f74c-d71c-4407-af05-db4fa6446550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# MODEL\n",
        "##########################\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(x)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes, grayscale):\n",
        "        self.inplanes = 64\n",
        "        if grayscale:\n",
        "            in_dim = 1\n",
        "        else:\n",
        "            in_dim = 3\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(4)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, (2. / n)**.5)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        logits = self.fc(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas\n",
        "\n",
        "def resnet34(num_classes, grayscale):\n",
        "    \"\"\"Constructs a ResNet-34 model.\"\"\"\n",
        "    model = ResNet(block=BasicBlock,\n",
        "                   layers=[3, 4, 6, 3],\n",
        "                   num_classes=num_classes,\n",
        "                   grayscale=grayscale)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ga8X6Av-niZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "# Initialize Cost, Model, and Optimizer\n",
        "###########################################\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "model = resnet34(NUM_CLASSES, GRAYSCALE)\n",
        "\n",
        "DEVICE = torch.device(\"cuda:%d\" % CUDA_DEVICE if CUDA_DEVICE >= 0 else \"cpu\")\n",
        "model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def compute_mae_and_mse(model, data_loader, device):\n",
        "    mae, mse, num_examples = 0., 0., 0\n",
        "    for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        logits, probas = model(features)\n",
        "        _, predicted_labels = torch.max(probas, 1)\n",
        "        num_examples += targets.size(0)\n",
        "        mae += torch.sum(torch.abs(predicted_labels - targets))\n",
        "        mse += torch.sum((predicted_labels - targets)**2)\n",
        "\n",
        "    mae = mae.float()/num_examples\n",
        "    mse = mse.float()/num_examples\n",
        "\n",
        "    return mae, mse\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "best_mae, best_rmse, best_epoch = 999, 999, -1\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "        features = features.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        # FORWARD AND BACK PROP\n",
        "        logits, probas = model(features)\n",
        "        cost = F.cross_entropy(logits, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cost.backward()\n",
        "\n",
        "        # UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "\n",
        "        # LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            s = ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
        "                 % (epoch+1, num_epochs, batch_idx,\n",
        "                     len(train_dataset)//BATCH_SIZE, cost))\n",
        "            print(s)\n",
        "            with open(LOGFILE, 'a') as f:\n",
        "                f.write('%s\\n' % s)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        test_mae, test_mse = compute_mae_and_mse(model, test_loader,\n",
        "                                                device=DEVICE)\n",
        "\n",
        "    if test_mae < best_mae:\n",
        "        best_mae, best_rmse, best_epoch = test_mae, torch.sqrt(test_mse), epoch\n",
        "        ########## SAVE MODEL #############\n",
        "        torch.save(model.state_dict(), os.path.join(OUT_PATH, 'best_model.pt'))\n",
        "\n",
        "    s = 'MAE/RMSE: | Current Test: %.2f/%.2f Ep. %d | Best Test : %.2f/%.2f Ep. %d' % (\n",
        "        test_mae, torch.sqrt(test_mse), epoch, best_mae, best_rmse, best_epoch)\n",
        "    print(s)\n",
        "    with open(LOGFILE, 'a') as f:\n",
        "        f.write('%s\\n' % s)\n",
        "\n",
        "    s = 'Time elapsed: %.2f min' % ((time.time() - start_time)/60)\n",
        "    print(s)\n",
        "    with open(LOGFILE, 'a') as f:\n",
        "        f.write('%s\\n' % s)\n",
        "\n",
        "    # Registro de métricas en WandB\n",
        "    wandb.log({\n",
        "        'epoch': epoch,\n",
        "        'train_cost': cost.item(),\n",
        "        'test_mae': test_mae.item(),\n",
        "        'test_rmse': torch.sqrt(test_mse).item(),\n",
        "        'best_mae': best_mae.item(),\n",
        "        'best_rmse': best_rmse.item()\n",
        "    })\n",
        "\n",
        "model.eval()\n",
        "with torch.set_grad_enabled(False):  # save memory during inference\n",
        "\n",
        "    train_mae, train_mse = compute_mae_and_mse(model, train_loader,\n",
        "                                               device=DEVICE)\n",
        "    test_mae, test_mse = compute_mae_and_mse(model, test_loader,\n",
        "                                             device=DEVICE)\n",
        "\n",
        "    s = 'MAE/RMSE: | Train: %.2f/%.2f | Test: %.2f/%.2f' % (\n",
        "        train_mae, torch.sqrt(train_mse), test_mae, torch.sqrt(test_mse))\n",
        "    print(s)\n",
        "    with open(LOGFILE, 'a') as f:\n",
        "        f.write('%s\\n' % s)\n",
        "\n",
        "    # Registro final de métricas en WandB\n",
        "    wandb.log({\n",
        "        'final_train_mae': train_mae.item(),\n",
        "        'final_train_rmse': torch.sqrt(train_mse).item(),\n",
        "        'final_test_mae': test_mae.item(),\n",
        "        'final_test_rmse': torch.sqrt(test_mse).item(),\n",
        "        'total_training_time': (time.time() - start_time)/60\n",
        "    })\n",
        "\n",
        "s = 'Total Training Time: %.2f min' % ((time.time() - start_time)/60)\n",
        "print(s)\n",
        "with open(LOGFILE, 'a') as f:\n",
        "    f.write('%s\\n' % s)\n",
        "\n",
        "s = 'Best MAE: %.2f | Best RMSE: %.2f | Best Epoch: %d' % (best_mae, best_rmse, best_epoch)\n",
        "print(s)\n",
        "with open(LOGFILE, 'a') as f:\n",
        "    f.write('%s\\n' % s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "PgeJ7YHhnn2E",
        "outputId": "2528c2b4-efa3-4d7d-dfd5-31b481ec8dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-25-3b1a3f159885>\", line 18, in __getitem__\n    img = Image.open(os.path.join(self.img_dir,\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3227, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/XNAPproject-grup_15-main/AFAD-Full/22/112/187552-0.jpg'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b9e9fee664de>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-25-3b1a3f159885>\", line 18, in __getitem__\n    img = Image.open(os.path.join(self.img_dir,\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3227, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/XNAPproject-grup_15-main/AFAD-Full/22/112/187552-0.jpg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## SAVE PREDICTIONS ######\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(OUT_PATH, 'best_model.pt')))\n",
        "model.eval()\n",
        "all_pred = []\n",
        "with torch.set_grad_enabled(False):\n",
        "    for batch_idx, (features, targets) in enumerate(test_loader):\n",
        "\n",
        "        features = features.to(DEVICE)\n",
        "        logits, probas = model(features)\n",
        "        predict_levels = probas > 0.5\n",
        "        predicted_labels = torch.sum(predict_levels, dim=1)\n",
        "        lst = [str(int(i)) for i in predicted_labels]\n",
        "        all_pred.extend(lst)\n",
        "\n",
        "with open(TEST_PREDICTIONS, 'w') as f:\n",
        "    all_pred = ','.join(all_pred)\n",
        "    f.write(all_pred)"
      ],
      "metadata": {
        "id": "XUE3qtAWoETM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}